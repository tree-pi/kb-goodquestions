{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## goal of this notebook\n",
    "The project is to ask questions like \"what is the largest city in america\". Thus we need to: \n",
    "- find the adj-noun pair (large - city)\n",
    "- maybe also find qualifier (*american* city)\n",
    "\n",
    "## how to do that\n",
    "1. predetermine \"words\" (i.e. nouns)\n",
    "2. find the most frequent adjs in front of that word by searching through sentences in a corpus\n",
    "\n",
    "## what do we got\n",
    "Tried quora questions as well as webtext and brown corpus in nltk tool kit. Quora questions are the best. \n",
    "Results in \"adj-noun.p\".\n",
    "However, unable to distinguish degradable VS absolute adjectives. Need that to produce valid questions!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.core.debugger import set_trace\n",
    "import collections\n",
    "import pickle\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "cachedStopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = ['mountain','population','company','area','income','employment','city']\n",
    "topics = ['country','income','animal','plant','city']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use the quora question pair corpus? -- can get the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qrsents = pickle.load(open('quorasents.p','rb'))\n",
    "qresults = searchsent(qrsents,words,topics, 'quora_sents.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mountain\n",
      "['highest' 'biggest' 'largest']\n",
      "population\n",
      "['Indian' 'Muslim' 'human' 'general' 'Chinese' 'small' 'Jewish' 'low'\n",
      " 'growing' 'largest' 'large' 'rural' 'male' 'lesser' 'heavy' 'higher'\n",
      " 'smallest' 'current' 'civilian' 'biggest' 'Russian' 'Hindu' 'increasing'\n",
      " 'young']\n",
      "company\n",
      "['best' 'good' 'limited' 'private' 'new' 'big' 'Indian' 'pharmaceutical'\n",
      " 'small' 'current' 'public' 'better' 'large' 'mobile' 'Chinese'\n",
      " 'registered' 'international' 'owned' 'financial' 'foreign' 'rental'\n",
      " 'successful' 'Japanese' 'listed' 'reliable' 'top' 'oldest' 'regular'\n",
      " 'Canadian' 'different' 'Private' 'bigger' 'corporate' 'first' 'held'\n",
      " 'worst' 'leading' 'profitable' 'legitimate' 'expensive' 'mechanical'\n",
      " 'related' 'offshore' 'normal' 'single' 'automotive' 'specific' 'analytic'\n",
      " 'valuable' 'valid' 'industrial' 'lost']\n",
      "area\n",
      "['surface' 'best' 'rural' 'public' 'light' 'local' 'residential' 'urban'\n",
      " 'pubic' 'good' 'general' 'safe' 'small' 'much' 'geographical' 'remote'\n",
      " 'expensive' 'sectional' 'large' 'private']\n",
      "income\n",
      "['passive' 'annual' 'much' 'gross' 'fixed' 'low' 'national' 'basic'\n",
      " 'rental' 'good' 'personal' 'residual' 'agricultural' 'fresher' 'foreign'\n",
      " 'second' 'decent' 'stable' 'high' 'net' 'potential' 'single' 'maximum'\n",
      " 'multiple' 'minimum' 'taxable' 'main' 'higher' 'federal' 'corporate'\n",
      " 'better' 'online' 'zero']\n",
      "employment\n",
      "['good' 'best' 'better' 'higher' 'self']\n",
      "city\n",
      "['best' 'better' 'beautiful' 'small' 'smart' 'safest' 'Indian' 'largest'\n",
      " 'big' 'good' 'biggest' 'dangerous' 'new' 'major' 'different' 'real'\n",
      " 'Australian' 'expensive' 'safe' 'large' 'oldest' 'specific' 'electronic'\n",
      " 'dead' 'cleanest' 'friendly' 'inner' 'developed' 'crowded' 'metropolitan'\n",
      " 'cosmopolitan' 'open' 'cheapest' 'Italian' 'Greek' 'Chinese' 'free'\n",
      " 'American']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "file must have a 'write' attribute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-d7a1fba90b17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'adj-noun.p'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: file must have a 'write' attribute"
     ]
    }
   ],
   "source": [
    "freas = []\n",
    "for kw in range(len(words)):\n",
    "    freas.append(reqadj([qresults[0][kw]]))\n",
    "    print(words[kw])\n",
    "    print(freas[kw])\n",
    "pickle.dump([words,freas],'adj-noun.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### just using weights in word net? too limited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('heavy.a.01'), Synset('light.a.01')]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('weight.n.01').attributes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try the brown corpus? -- still not large enough, missing a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategic population\n",
      "uptown area\n",
      "immediate area\n",
      "residential area\n",
      "possible income\n",
      "practical employment\n",
      "full employment\n",
      "gross income\n",
      "Net income\n",
      "Federal income\n",
      "higher area\n",
      "wide area\n",
      "metropolitan area\n",
      "downtown area\n",
      "personal income\n",
      "ample area\n",
      "large population\n",
      "large population\n",
      "raw population\n",
      "Catholic population\n",
      "religious population\n",
      "important area\n",
      "national population\n",
      "American population\n",
      "white population\n",
      "geographic area\n",
      "cross-sectional area\n",
      "wooded area\n",
      "living area\n",
      "living area\n",
      "industrial area\n",
      "large area\n",
      "useful area\n",
      "light area\n",
      "black area\n",
      "geographical area\n",
      "minimal income\n",
      "urban area\n",
      "steep mountain\n",
      "civilian population\n",
      "known mountain\n",
      "general population\n",
      "annual income\n",
      "industrial area\n",
      "gross income\n",
      "American population\n",
      "respectable income\n",
      "Jewish population\n",
      "metropolitan area\n",
      "volcanic mountain\n",
      "poor area\n",
      "residential area\n",
      "personal income\n",
      "maximum employment\n",
      "small area\n",
      "small area\n",
      "homogeneous population\n",
      "graduated income\n",
      "regular employment\n",
      "open area\n",
      "Celtic population\n",
      "Celtic population\n",
      "free employment\n",
      "Atlantic area\n",
      "immediate area\n",
      "national income\n",
      "lower employment\n",
      "central area\n",
      "federal income\n",
      "ordinary income\n",
      "federal income\n",
      "wide area\n",
      "gross income\n",
      "gross income\n",
      "gross income\n",
      "gross income\n",
      "net income\n",
      "Net income\n",
      "operating income\n",
      "net income\n",
      "cross-sectional area\n",
      "surface area\n",
      "static population\n",
      "extensive area\n",
      "greater area\n",
      "wider area\n",
      "small area\n",
      "urban area\n",
      "larger population\n",
      "larger population\n",
      "illiterate population\n",
      "African population\n",
      "current population\n",
      "industrial area\n",
      "complex area\n",
      "territorial area\n",
      "occupational employment\n",
      "better employment\n",
      "future employment\n",
      "sensitive area\n",
      "public employment\n",
      "specific area\n",
      "whole area\n",
      "unearned income\n",
      "surface area\n",
      "bacterial population\n",
      "residential area\n",
      "geographical area\n",
      "Large area\n",
      "effective area\n",
      "effective area\n",
      "Effective employment\n",
      "cross-sectional area\n",
      "seasonal employment\n",
      "steady income\n",
      "black area\n",
      "private area\n",
      "nebulous area\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "results=searchsent(brown.sents(),words,topics, 'brown_sents.p',doprint=False)\n",
    "\n",
    "freas = []\n",
    "for kw in range(len(words)):\n",
    "    freas.append(reqadj([results[0][kw]]))\n",
    "    print(words[kw])\n",
    "    print(freas[kw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reqadj(adjs):\n",
    "    # return adjs in descending frequency in the given data set. only words with freq >1 are considered\n",
    "    [awords,freq]=np.unique(adjs, return_counts=True)\n",
    "    awords = awords[freq>1]\n",
    "    freq=freq[freq>1]\n",
    "    freqa=awords[np.argsort(-freq)]\n",
    "    return freqa\n",
    "def isadj(word):\n",
    "    for kk in wn.synsets(word):\n",
    "        if kk.pos()=='a':\n",
    "            return True\n",
    "    return False\n",
    "def searchsent(sentences,words,topics,fname,doprint=False):\n",
    "    wsents = [[] for k in range(len(words))]\n",
    "    tsents = [[] for k in range(len(topics))]\n",
    "    wbef = [[] for k in range(len(words))]\n",
    "    for st in sentences:\n",
    "        if type(st) is not list:\n",
    "            swords = tokenizer.tokenize(st)\n",
    "        else:\n",
    "            swords = st\n",
    "        for kword in range(len(words)):\n",
    "            word = words[kword]\n",
    "            if word in swords:        \n",
    "                wsents[kword].append(swords)\n",
    "                widx=swords.index(word)\n",
    "                if widx>0: #if it's the beginning of sentence, skip this sentence\n",
    "                    wordbef = swords[widx-1]\n",
    "                    if wordbef not in stopwords.words(\"english\"):               \n",
    "                        \"\"\"try:\n",
    "                            wordpos = wn.synsets(wordbef)[0].pos()\n",
    "                        except:\n",
    "                            continue\"\"\"\n",
    "                        if isadj(wordbef):\n",
    "                            if doprint:\n",
    "                                print(wordbef+' '+word)\n",
    "                            wbef[kword].append(wordbef)\n",
    "\n",
    "        for kt in range(len(topics)): # todo: search for broader topics to look for other critical nouns\n",
    "            topic = topics[kt]\n",
    "            if topic in swords:\n",
    "                tsents[kt].append(swords)\n",
    "    pickle.dump([wsents,tsents],open(fname,'wb'))\n",
    "    return [wbef,wsents,tsents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty area\n",
      "content area\n",
      "content area\n",
      "black area\n",
      "content area\n",
      "content area\n",
      "Empty area\n"
     ]
    }
   ],
   "source": [
    "# obsolete: webtext is not a good corpora\n",
    "sentns = []\n",
    "for fileid in webtext.fileids():\n",
    "    sentns.append(webtext.sents(fileid))\n",
    "websents = [] # join them together\n",
    "for cs in sentns:\n",
    "    for s in cs:\n",
    "        websents.append(s)\n",
    "fname='webtext_sents.p'\n",
    "results=searchsent(websents[:1111],words,topics, fname)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
